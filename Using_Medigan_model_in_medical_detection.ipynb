{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOZTtf4Yyi8MrQmUnSQRjUE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["### __Getting Started__"],"metadata":{"id":"gk6_ZfThfKqV"}},{"cell_type":"markdown","source":[" __import__"],"metadata":{"id":"i8Tw8grMfPmP"}},{"cell_type":"code","source":["!pip install medigan"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QNqAOZ-gfnmp","executionInfo":{"status":"ok","timestamp":1742798057577,"user_tz":-180,"elapsed":2915,"user":{"displayName":"Irura Jack","userId":"08313893547887242679"}},"outputId":"f343380d-1ab3-4fa6-d6af-034e49c12bd3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: medigan in /usr/local/lib/python3.11/dist-packages (1.0.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from medigan) (4.67.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from medigan) (2.32.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from medigan) (2.6.0+cu124)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from medigan) (2.0.2)\n","Requirement already satisfied: PyGithub in /usr/local/lib/python3.11/dist-packages (from medigan) (2.6.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from medigan) (3.10.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->medigan) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->medigan) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->medigan) (4.56.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->medigan) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->medigan) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->medigan) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->medigan) (3.2.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->medigan) (2.8.2)\n","Requirement already satisfied: pynacl>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from PyGithub->medigan) (1.5.0)\n","Requirement already satisfied: pyjwt>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub->medigan) (2.10.1)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from PyGithub->medigan) (4.12.2)\n","Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from PyGithub->medigan) (2.3.0)\n","Requirement already satisfied: Deprecated in /usr/local/lib/python3.11/dist-packages (from PyGithub->medigan) (1.2.18)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->medigan) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->medigan) (3.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->medigan) (2025.1.31)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->medigan) (3.18.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->medigan) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->medigan) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->medigan) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->medigan) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->medigan) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->medigan) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->medigan) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->medigan) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->medigan) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->medigan) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->medigan) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->medigan) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->medigan) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->medigan) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->medigan) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->medigan) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->medigan) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->medigan) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->medigan) (1.3.0)\n","Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub->medigan) (43.0.3)\n","Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from pynacl>=1.4.0->PyGithub->medigan) (1.17.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->medigan) (1.17.0)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from Deprecated->PyGithub->medigan) (1.17.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->medigan) (3.0.2)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub->medigan) (2.22)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8QKTPMIGfFr1"},"outputs":[],"source":["from pathlib import Path\n","\n","import matplotlib.pyplot as plt\n","import medigan\n","import torch\n","import torch.optim as optim\n","import torchvision\n","from torchvision.io import read_image\n","from torchvision.utils import make_grid\n","from tqdm.notebook import tqdm"]},{"cell_type":"markdown","source":["Since we'll be working with large models in this notebook, let's make sure we're using GPUs."],"metadata":{"id":"O7nJeZYng0Ru"}},{"cell_type":"code","source":["if torch.cuda.is_available():\n","    device = \"cuda\"\n","elif torch.backends.mps.is_available():\n","    device = \"mps\"\n","else:\n","    device = \"cpu\"\n","\n","print(f\"Using {device} device.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C0D0D2-WgMqH","executionInfo":{"status":"ok","timestamp":1742798204668,"user_tz":-180,"elapsed":40,"user":{"displayName":"Irura Jack","userId":"08313893547887242679"}},"outputId":"5c960b45-f61d-4b61-ad68-68b73ac8c4e0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda device.\n"]}]},{"cell_type":"markdown","source":["### __Medigan Models__"],"metadata":{"id":"nZ5S4gUUgLDY"}},{"cell_type":"markdown","source":["Medigan is a pretrained GAN."],"metadata":{"id":"T-urJeeShJyK"}},{"cell_type":"markdown","source":["It's a whole collection of GANs. `medigan.Generators` gives us access to the GANs in the package. Here we can see the available ones, which you can also see [here](https://github.com/RichardObi/medigan?tab=readme-ov-file#available-models)."],"metadata":{"id":"NrghY1mehYxb"}},{"cell_type":"code","source":["generators = medigan.Generators()\n","\n","generators.list_models()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-EpR-Q18hj3n","executionInfo":{"status":"ok","timestamp":1742798209783,"user_tz":-180,"elapsed":47,"user":{"displayName":"Irura Jack","userId":"08313893547887242679"}},"outputId":"7ecaf9f6-c9b4-4677-c2a9-a799b19bc61e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['00001_DCGAN_MMG_CALC_ROI',\n"," '00002_DCGAN_MMG_MASS_ROI',\n"," '00003_CYCLEGAN_MMG_DENSITY_FULL',\n"," '00004_PIX2PIX_MMG_MASSES_W_MASKS',\n"," '00005_DCGAN_MMG_MASS_ROI',\n"," '00006_WGANGP_MMG_MASS_ROI',\n"," '00007_INPAINT_BRAIN_MRI',\n"," '00008_C-DCGAN_MMG_MASSES',\n"," '00009_PGGAN_POLYP_PATCHES_W_MASKS',\n"," '00010_FASTGAN_POLYP_PATCHES_W_MASKS',\n"," '00011_SINGAN_POLYP_PATCHES_W_MASKS',\n"," '00012_C-DCGAN_MMG_MASSES',\n"," '00013_CYCLEGAN_MMG_DENSITY_OPTIMAM_MLO',\n"," '00014_CYCLEGAN_MMG_DENSITY_OPTIMAM_CC',\n"," '00015_CYCLEGAN_MMG_DENSITY_CSAW_MLO',\n"," '00016_CYCLEGAN_MMG_DENSITY_CSAW_CC',\n"," '00017_DCGAN_XRAY_LUNG_NODULES',\n"," '00018_WGANGP_XRAY_LUNG_NODULES',\n"," '00019_PGGAN_CHEST_XRAY',\n"," '00020_PGGAN_CHEST_XRAY',\n"," '00021_CYCLEGAN_BRAIN_MRI_T1_T2',\n"," '00022_WGAN_CARDIAC_AGING',\n"," '00023_PIX2PIXHD_BREAST_DCEMRI']"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["Each one has an id number (the first number in the name) and a somewhat descriptive name. But we'll want more information to decide which one we want. We can get the configuration for the first one, this tells us a lot about what the model does. The `model_id` can be either the id number or the whole name."],"metadata":{"id":"BABXHC9sht_a"}},{"cell_type":"code","source":["model_00001_config = generators.get_config_by_id(model_id=1)\n","model_00001_config"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2lHh5J3_hzcG","executionInfo":{"status":"ok","timestamp":1742798218694,"user_tz":-180,"elapsed":34,"user":{"displayName":"Irura Jack","userId":"08313893547887242679"}},"outputId":"cbf1bc87-98f4-43cf-c4d3-12a7ff514716"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'execution': {'package_name': '00001_DCGAN_MMG_CALC_ROI',\n","  'package_link': 'https://zenodo.org/record/7031735/files/00001_DCGAN_MMG_CALC_ROI.zip?download=1',\n","  'model_name': 'DCGAN',\n","  'extension': '.pt',\n","  'image_size': [128, 128],\n","  'dependencies': ['numpy', 'Path', 'torch', 'opencv-contrib-python-headless'],\n","  'generate_method': {'name': 'generate',\n","   'args': {'base': ['model_file',\n","     'num_samples',\n","     'output_path',\n","     'save_images'],\n","    'custom': {'image_size': 128}},\n","   'input_latent_vector_size': 100}},\n"," 'selection': {'performance': {'SSIM': None,\n","   'MSE': None,\n","   'NSME': None,\n","   'PSNR': None,\n","   'IS': None,\n","   'turing_test': None,\n","   'FID_no_images': 1000,\n","   'FID': 67.6,\n","   'FID_ratio': 0.497,\n","   'FID_RADIMAGENET': 1.27,\n","   'FID_RADIMAGENET_ratio': 0.197,\n","   'CLF_delta': None,\n","   'SEG_delta': None,\n","   'CLF': {'trained_on_fake': {},\n","    'trained_on_real_and_fake': {},\n","    'trained_on_real': {}},\n","   'SEG': {'trained_on_fake': {},\n","    'trained_on_real_and_fake': {},\n","    'trained_on_real': {}},\n","   'DET': {'trained_on_fake': {},\n","    'trained_on_real_and_fake': {},\n","    'trained_on_real': {}}},\n","  'use_cases': ['classification'],\n","  'organ': ['breast', 'breasts', 'chest'],\n","  'modality': ['MMG',\n","   'Mammography',\n","   'Mammogram',\n","   'full-field digital',\n","   'full-field digital MMG',\n","   'full-field MMG',\n","   'full-field Mammography',\n","   'digital Mammography',\n","   'digital MMG',\n","   'x-ray mammography'],\n","  'vendors': [],\n","  'centres': [],\n","  'function': ['noise to image',\n","   'image generation',\n","   'unconditional generation',\n","   'data augmentation'],\n","  'condition': [],\n","  'dataset': ['INbreast'],\n","  'augmentations': ['crop and resize', 'horizontal flip', 'vertical flip'],\n","  'generates': ['calcification',\n","   'calcifications',\n","   'calcification roi',\n","   'calcification ROI',\n","   'calcification images',\n","   'calcification region of interest'],\n","  'height': 128,\n","  'width': 128,\n","  'depth': None,\n","  'type': 'DCGAN',\n","  'license': 'MIT',\n","  'dataset_type': 'public',\n","  'privacy_preservation': None,\n","  'tags': ['Mammogram',\n","   'Mammography',\n","   'Digital Mammography',\n","   'Full field Mammography',\n","   'Full-field Mammography',\n","   '128x128',\n","   '128 x 128',\n","   'MammoGANs',\n","   'Microcalcification',\n","   'Microcalcifications'],\n","  'year': '2021'},\n"," 'description': {'title': 'DCGAN Model for Mammogram Calcification Region of Interest Generation (Trained on INbreast)',\n","  'provided_date': '12th May 2021',\n","  'trained_date': 'May 2021',\n","  'provided_after_epoch': 300,\n","  'version': '0.0.1',\n","  'publication': None,\n","  'doi': ['10.5281/zenodo.5187714'],\n","  'inputs': ['image_size: default=128, help=128 is the image size that works with the supplied checkpoint.'],\n","  'comment': 'A deep convolutional generative adversarial network (DCGAN) that generates regions of interest (ROI) of mammograms containing benign and/or malignant calcifications. Pixel dimensions are 128x128. The DCGAN was trained on ROIs from the INbreast dataset (Moreira et al, 2012). The uploaded ZIP file contains the files dcgan.pt (model weights), __init__.py (image generation method and utils), a README.md, and the GAN model architecture (in pytorch) below the /src folder. Kernel size=6 used in DCGAN discriminator.'}}"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["There's a lot of good information here. This model generates images of mammograms, of size $128$ x $128$ pixels. Let's look at another one."],"metadata":{"id":"06p7HeEMiM5X"}},{"cell_type":"markdown","source":["**Task 5.3.1:** Make a new dictionary called `model_00001_info` that has just the `organ`, `modality`, `tags` keys from within the `selection` key in `model_00001_config`."],"metadata":{"id":"3X1EtrbniOfs"}},{"cell_type":"code","source":["model_00001_info = {\n","    k: model_00001_config[\"selection\"][k] for k in [\"organ\", \"modality\", \"tags\"]\n","}\n","model_00001_info\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xn8gAmveiqYh","executionInfo":{"status":"ok","timestamp":1742798222497,"user_tz":-180,"elapsed":68,"user":{"displayName":"Irura Jack","userId":"08313893547887242679"}},"outputId":"e67096bc-8dc0-4fa3-faaa-ebdd4cfddff4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'organ': ['breast', 'breasts', 'chest'],\n"," 'modality': ['MMG',\n","  'Mammography',\n","  'Mammogram',\n","  'full-field digital',\n","  'full-field digital MMG',\n","  'full-field MMG',\n","  'full-field Mammography',\n","  'digital Mammography',\n","  'digital MMG',\n","  'x-ray mammography'],\n"," 'tags': ['Mammogram',\n","  'Mammography',\n","  'Digital Mammography',\n","  'Full field Mammography',\n","  'Full-field Mammography',\n","  '128x128',\n","  '128 x 128',\n","  'MammoGANs',\n","  'Microcalcification',\n","  'Microcalcifications']}"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["**Task 5.3.2:** Extract the `title` and `comment` keys from the `description` key in `model_00001_config` and add it to the `model_00001_info` dictionary."],"metadata":{"id":"BR4EDWrYis-O"}},{"cell_type":"code","source":["model_00001_info['title'] = model_00001_config['description']['title']\n","\n","model_00001_info['comment'] = model_00001_config['description']['title']"],"metadata":{"id":"Ulefq3Aaiyer"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["There's a lot of good information here. This model generates images of mammograms, of size $128$ x $128$ pixels. Let's look at another one."],"metadata":{"id":"Xa_VRXCWklwU"}},{"cell_type":"markdown","source":["**Task 5.3.3:** Get the configuration for model $7$, and extract `organ`, `modality`, `tags` keys from within the `selection` key and the `title` and `comment` keys from the `description` key. Store that information in `model_00007_info`."],"metadata":{"id":"xX9J8alkkqW8"}},{"cell_type":"code","source":["model_00007_config = generators.get_config_by_id(model_id=7)\n","\n","model_00007_info = {key:model_00007_config['selection'][key] for key in ['organ','modality', 'tags']}\n","model_00007_info.update({key:model_00007_config['description'][key]\n","                        for key in ['title','comment']})\n","model_00007_info"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jHJIkAYWnKHk","executionInfo":{"status":"ok","timestamp":1742798228611,"user_tz":-180,"elapsed":7,"user":{"displayName":"Irura Jack","userId":"08313893547887242679"}},"outputId":"fad12d0a-dcc4-45b9-9e2e-20b13930267b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'organ': ['brain', 'cranial', 'head'],\n"," 'modality': ['MRI',\n","  'Cranial MRI',\n","  'Brain MRI',\n","  'Flair',\n","  'T1',\n","  'T1c',\n","  'T1 contrast-enhanced',\n","  'T2'],\n"," 'tags': ['Brain',\n","  'Tumor',\n","  'MRI Generation',\n","  'Inpainting',\n","  'Brain MRI Synthesis',\n","  'Concentric Circle',\n","  'Tumor Inpainting',\n","  'Tumor Grade',\n","  'Tumor Grading',\n","  'Cross-Modality',\n","  'Multi-modal synthesis'],\n"," 'title': 'Tumor Inpainting Model for Generation of Flair, T1, T1c, T2 Brain MRI Images (Trained on BRATS)',\n"," 'comment': 'A Generative adversarial network (GAN) for Inpainting tumors (based on concentric circle-based tumor grade masks) into multi-modal MRI images (Flair, T1, T1c, T2) with dimensions 256x256. Model was trained on BRATS MRI Dataset (Menze et al). For more information, see publication (https://doi.org/10.1002/mp.14701). Model comes with example input image folders. Apart from that, the uploaded ZIP file contains the model checkpoint files .pth (model weight), __init__.py (image generation method and utils), a requirements.txt, the MEDIGAN metadata.json. The proposed method synthesizes brain tumor images from normal brain images and concentric circles that are simplified tumor masks. The tumor masks are defined by complex features, such as grade, appearance, size, and location. Thus, these features of the tumor masks are condensed and simplified to concentric circles. In the proposed method, the user-defined concentric circles are converted to various tumor masks through deep neural networks. The normal brain images are masked by the tumor mask, and the masked region is inpainted with the tumor images synthesized by the deep neural networks. Also see original repository at: https://github.com/KSH0660/BrainTumor'}"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["This one is creating images of brain MRIs with tumors.\n","\n","While this is very valuable information, we aren't going to want to look through this for every model to find the kind of images we want. Medigan provides us with the ability to search through the configurations. We have to say which key we want to search over - usually `organ`, `modality`, or `tags`, but you can search the others too. Let's see which models look at the brain, in the `organ` key. We'll set it to not be case-sensitive, so we don't have to worry about capitalization."],"metadata":{"id":"o8AS2m8vnQeu"}},{"cell_type":"code","source":["key = \"organ\"\n","value = \"brain\"\n","\n","found_models = generators.get_models_by_key_value_pair(\n","    key1=key, value1=value, is_case_sensitive=False\n",")\n","print(found_models)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uNPfmOFOngq5","executionInfo":{"status":"ok","timestamp":1742798233884,"user_tz":-180,"elapsed":7,"user":{"displayName":"Irura Jack","userId":"08313893547887242679"}},"outputId":"daab8b60-e728-4649-d6da-11b9c5f1a505"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'model_id': '00007_INPAINT_BRAIN_MRI', 'organ': 'brain'}, {'model_id': '00021_CYCLEGAN_BRAIN_MRI_T1_T2', 'organ': 'brain'}]\n"]}]},{"cell_type":"markdown","source":["The search has returned a list of dictionaries, with each dictionary one model that matched. We got only two models back, at least as of the time this notebook was written.\n","\n","Let's see which models are about mammograms."],"metadata":{"id":"a-lZwUhinoNC"}},{"cell_type":"markdown","source":["**Task 5.3.4:** Search for models that have `mammogram` in their `tags`. Make sure the search isn't case sensitive."],"metadata":{"id":"feVyugLWnr4m"}},{"cell_type":"code","source":["key =\"tags\"\n","value = \"mammogram\"\n","\n","found_models = generators.get_models_by_key_value_pair(\n","    key1=key, value1=value, is_case_sensitive=False\n",")\n","\n","print(found_models)\n","print()\n","print(f\"Found {len(found_models)} models\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aD9mud6apPVL","executionInfo":{"status":"ok","timestamp":1742798237044,"user_tz":-180,"elapsed":23,"user":{"displayName":"Irura Jack","userId":"08313893547887242679"}},"outputId":"dd4820b3-c85d-42b3-89d1-53dfa9453f07"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'model_id': '00001_DCGAN_MMG_CALC_ROI', 'tags': 'mammogram'}, {'model_id': '00002_DCGAN_MMG_MASS_ROI', 'tags': 'mammogram'}, {'model_id': '00003_CYCLEGAN_MMG_DENSITY_FULL', 'tags': 'mammogram'}, {'model_id': '00004_PIX2PIX_MMG_MASSES_W_MASKS', 'tags': 'mammogram'}, {'model_id': '00005_DCGAN_MMG_MASS_ROI', 'tags': 'mammogram'}, {'model_id': '00006_WGANGP_MMG_MASS_ROI', 'tags': 'mammogram'}, {'model_id': '00008_C-DCGAN_MMG_MASSES', 'tags': 'mammogram'}, {'model_id': '00012_C-DCGAN_MMG_MASSES', 'tags': 'mammogram'}, {'model_id': '00013_CYCLEGAN_MMG_DENSITY_OPTIMAM_MLO', 'tags': 'mammogram'}, {'model_id': '00014_CYCLEGAN_MMG_DENSITY_OPTIMAM_CC', 'tags': 'mammogram'}, {'model_id': '00015_CYCLEGAN_MMG_DENSITY_CSAW_MLO', 'tags': 'mammogram'}, {'model_id': '00016_CYCLEGAN_MMG_DENSITY_CSAW_CC', 'tags': 'mammogram'}]\n","\n","Found 12 models\n"]}]},{"cell_type":"markdown","source":["We can then fetch a model's configuration to learn more about it. The `get_config_by_id` we used before understands the `model_id` we get from the search. We've already seen the first one, so let's look at the second one that the search found."],"metadata":{"id":"sowN9oe2pVCK"}},{"cell_type":"code","source":["model_id = found_models[1][\"model_id\"]\n","\n","model_config = generators.get_config_by_id(model_id=model_id)\n","\n","print(\"Model's description:\")\n","model_config[\"description\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rDXt0whZpWco","executionInfo":{"status":"ok","timestamp":1742798245018,"user_tz":-180,"elapsed":10,"user":{"displayName":"Irura Jack","userId":"08313893547887242679"}},"outputId":"ed1b8dd6-c54b-4c86-af32-5f627a1e9ab7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model's description:\n"]},{"output_type":"execute_result","data":{"text/plain":["{'title': 'DCGAN Model for Mammogram Mass Region of Interest Generation (Trained on OPTIMAM)',\n"," 'provided_date': None,\n"," 'trained_date': None,\n"," 'provided_after_epoch': None,\n"," 'version': None,\n"," 'publication': None,\n"," 'doi': ['10.5281/zenodo.5188557', '10.1117/12.2543506', '10.1117/12.2560473'],\n"," 'inputs': [],\n"," 'comment': 'A deep convolutional generative adversarial network (DCGAN) that generates regions of interest (ROI) of mammograms containing benign and/or malignant masses. Pixel dimensions are 128x128. The DCGAN was trained on ROIs from the Optimam dataset (Halling-Brown et al, 2014). The uploaded ZIP file contains the files malign_mass_gen (model weights), and __init__.py (image generation method and pytorch GAN model architecture). Kernel size=6 used in DCGAN discriminator.'}"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["**Task 5.3.5:** Get the model configuration for the third model we got in our search."],"metadata":{"id":"jq9_cBj1pbLQ"}},{"cell_type":"code","source":["model_id = found_models[2]['model_id']\n","\n","model_config = generators.get_config_by_id(model_id)\n","\n","print(\"Model's description:\")\n","model_config[\"description\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SPfZO0G0rcii","executionInfo":{"status":"ok","timestamp":1742798247604,"user_tz":-180,"elapsed":31,"user":{"displayName":"Irura Jack","userId":"08313893547887242679"}},"outputId":"ebce32ee-b6c8-4693-8d7e-7dc22777bd71"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model's description:\n"]},{"output_type":"execute_result","data":{"text/plain":["{'title': 'CycleGAN Model for Low-to-High Brest Density Mammograms Translation (Trained on BCDR)',\n"," 'provided_date': '12th Sep 2021',\n"," 'trained_date': 'Sep 2021',\n"," 'provided_after_epoch': 100,\n"," 'version': '0.0.1',\n"," 'publication': None,\n"," 'doi': ['https://doi.org/10.48550/arXiv.2209.09809'],\n"," 'inputs': ['input_path: default=models/00003_CYCLEGAN_MMG_DENSITY_FULL/images, help=the path to .png mammogram images that are translated from low to high breast density or vice versa',\n","  'image_size: default=[1332, 800], help=list with image height and width. Images are rescaled to these pixel dimensions.',\n","  'gpu_id: default=0, help=the gpu to run the model on.',\n","  'translate_all_images: default=False, help=flag to override num_samples in case the user wishes to translate all images in the specified input_path folder.'],\n"," 'comment': 'A cycle generative adversarial network (CycleGAN) that generates mammograms with high breast density from an original mammogram e.g. with low-breast density. The CycleGAN was trained using normal (without pathologies) digital mammograms from BCDR dataset (Lopez, M. G., et al. 2012). The uploaded ZIP file contains the files CycleGAN_high_density.pth (model weights), __init__.py (image generation method and utils) and the GAN model architecture (in pytorch) below the /src folder.'}"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["We're not limited to searching for a single value. We can search over multiple terms at once with `find_matching_models_by_values`, and give a list of values. We don't need to specify a key now, as it will look through all the keys. It returns any model that has matches for all of our values.\n","\n","Let's narrow down our mammograms to ones that are also about calcification."],"metadata":{"id":"Lxc08Fgrrkeg"}},{"cell_type":"code","source":["values_list = [\"mammogram\", \"calcification\"]\n","found_models = generators.find_matching_models_by_values(\n","    values=values_list,\n","    is_case_sensitive=False,\n",")\n","print(f\"Found models: {found_models}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G4Ib1Tc0rvyx","executionInfo":{"status":"ok","timestamp":1742798249842,"user_tz":-180,"elapsed":6,"user":{"displayName":"Irura Jack","userId":"08313893547887242679"}},"outputId":"504e7e7e-c7a1-4f74-d418-e5c1ba4ead2d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found models: [ModelMatchCandidate(model_id=00001_DCGAN_MMG_CALC_ROI, is_match=True, operator: AND, target_values=['mammogram', 'calcification'])]\n"]}]},{"cell_type":"markdown","source":["Just one response this time. The result comes back a little different than before. Instead of a list of dictionaries, we get a list of `ModelMatchCandidate` objects. They have information about the match, but all we're concerned about is the `model_id`. Let's grab that for later."],"metadata":{"id":"XZpLQXApr0mU"}},{"cell_type":"code","source":["model_id_mammogram = found_models[0].model_id\n","\n","model_id_mammogram"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"U9QZpAZSr5Cd","executionInfo":{"status":"ok","timestamp":1742798253136,"user_tz":-180,"elapsed":73,"user":{"displayName":"Irura Jack","userId":"08313893547887242679"}},"outputId":"f4447bcc-5ed4-44b1-c7cc-beecb8fbb57a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'00001_DCGAN_MMG_CALC_ROI'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["<div class=\"alert alert-info\" role=\"alert\">\n","We can also use this tool to get a <i>broader</i> search. By default it looks for things that match all of our values. It checks if a model matches each value, then performs a logical <tt>AND</tt> operation. This will only return a model if all of the matches came back with <tt>True</tt>. We could instead set it to use a logical <tt>OR</tt> operation by giving it an additional argument <tt>target_values_operator=\"OR\"</tt>. Then it would return a model if <i>any</i> of the values came back with <tt>True</tt>. This would let us find, for example, everything with the terms chest, lung, and thorax if we were worried models were labeled with different terms.\n","</div>"],"metadata":{"id":"CAcIL6VQsBK4"}},{"cell_type":"markdown","source":["Let's find a model that gives lung X-rays. Let's look for high resolution images, so we have something nice to display on the screen when we use it to generate images."],"metadata":{"id":"oG_GZkGvsQV0"}},{"cell_type":"markdown","source":["**Task 5.3.6:** Search for a model that is marked with `lung`, `xray` and `1024` (for $1024$ by $1024$ images, which will be easier for us as humans to understand). There should be only one. Save the model's id to `lung_xray_id`."],"metadata":{"id":"R1PMlub1sftk"}},{"cell_type":"code","source":["values_list =['lung','xray',1024]\n","\n","found_models = generators.find_matching_models_by_values(values=values_list,is_case_sensitive=False)\n","\n","lung_xray_id = found_models[0].model_id\n","\n","print(f\"Lung x-ray model ID: {lung_xray_id}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K3Cb46g1uv4q","executionInfo":{"status":"ok","timestamp":1742798256764,"user_tz":-180,"elapsed":17,"user":{"displayName":"Irura Jack","userId":"08313893547887242679"}},"outputId":"922956b4-631f-4dde-a7f4-7d0c1c56d405"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Lung x-ray model ID: 00019_PGGAN_CHEST_XRAY\n"]}]},{"cell_type":"markdown","source":["### __Generating Images__"],"metadata":{"id":"PpBiMlSxu1Ro"}},{"cell_type":"markdown","source":["Now that we've picked a model, we can ask it to generate images for us. We do this with `generate`. We tell it which model and how many images (samples) to generate. It will go and download the model, set it up, then run it to create what we've asked for. By default, it creates the images as files and saves them to `output_path`, but we can also get them as a `numpy` array.\n","\n","Let's erase anything we may have generated in the past."],"metadata":{"id":"ktf4oYk7u4Bo"}},{"cell_type":"code","source":["! rm -rf output"],"metadata":{"id":"6xtONfxlu9GF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We'll need to tell it where to save the outputs. It defaults to a directory with the model name and a timestamp, but we'll set it to something more memorable."],"metadata":{"id":"ooQIbO2FvJks"}},{"cell_type":"markdown","source":["**Task 5.3.7:** Create a path to the directory `output/sample_lung`. Use `Pathlib`."],"metadata":{"id":"WVtSjJT2vRA0"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NUHGwRWpw9TF","executionInfo":{"status":"ok","timestamp":1742798265094,"user_tz":-180,"elapsed":2041,"user":{"displayName":"Irura Jack","userId":"08313893547887242679"}},"outputId":"9ad66451-07b1-416e-f68a-620a6de6b28d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["output_dir =Path(\"/content/drive/My Drive/Medical Data in Spain\")\n","sample_dir =output_dir / \"sample_lung\"\n","\n","print(sample_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VUTbppqMxGOQ","executionInfo":{"status":"ok","timestamp":1742798268876,"user_tz":-180,"elapsed":43,"user":{"displayName":"Irura Jack","userId":"08313893547887242679"}},"outputId":"f4c98e4a-6cc1-4343-891c-c0c25b696200"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/Medical Data in Spain/sample_lung\n"]}]},{"cell_type":"markdown","source":["We'll generate $8$ lung x-ray images by setting `num_samples=8`. It should take a few seconds to create the images."],"metadata":{"id":"929cElNWxY2e"}},{"cell_type":"markdown","source":["Manually set the betas when initializing the optimizer in the model code:"],"metadata":{"id":"yp05CVEHyvs1"}},{"cell_type":"code","source":["generators.generate(\n","    model_id=lung_xray_id,\n","    num_samples=8,\n","    output_path=sample_dir,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"rJqbrPfnxdQH","executionInfo":{"status":"error","timestamp":1742798272434,"user_tz":-180,"elapsed":909,"user":{"displayName":"Irura Jack","userId":"08313893547887242679"}},"outputId":"ee2236a9-73ad-424d-acfb-7e6f5080cdd4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1 [00:00<?, ?it/s]ERROR:root:Error while trying to generate 8 images with model models/00019_PGGAN_CHEST_XRAY/model.pt: betas must be either both floats or both Tensors\n","  0%|          | 0/1 [00:00<?, ?it/s]\n","ERROR:root:00019_PGGAN_CHEST_XRAY: Error while trying to generate images with model models/00019_PGGAN_CHEST_XRAY/model.pt: betas must be either both floats or both Tensors\n"]},{"output_type":"error","ename":"ValueError","evalue":"betas must be either both floats or both Tensors","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-c255ce35ac81>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m generators.generate(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlung_xray_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/medigan/generators.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, model_id, num_samples, output_path, save_images, is_gen_function_returned, install_dependencies, **kwargs)\u001b[0m\n\u001b[1;32m    794\u001b[0m             \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstall_dependencies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstall_dependencies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m         )\n\u001b[0;32m--> 796\u001b[0;31m         return model_executor.generate(\n\u001b[0m\u001b[1;32m    797\u001b[0m             \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m             \u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/medigan/execute_model/model_executor.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, num_samples, output_path, save_images, is_gen_function_returned, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m                 \u001b[0;34mf\"{self.serialised_model_file_path}: {e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     def _prepare_generate_method_args(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/medigan/execute_model/model_executor.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, num_samples, output_path, save_images, is_gen_function_returned, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m                     \u001b[0mprepared_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"output_path\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_path\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mgenerate_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mprepared_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/models/00019_PGGAN_CHEST_XRAY/__init__.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(model_file, num_samples, output_path, save_images, input_latent_vector)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;34mf\"Error while trying to generate {num_samples} images with model {model_file}: {e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         )\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/models/00019_PGGAN_CHEST_XRAY/__init__.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(model_file, num_samples, output_path, save_images, input_latent_vector)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         image_list = image_generator(\n\u001b[0m\u001b[1;32m    106\u001b[0m             \u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/models/00019_PGGAN_CHEST_XRAY/__init__.py\u001b[0m in \u001b[0;36mimage_generator\u001b[0;34m(model_path, device, num_samples, input_latent_vector)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# load the model's weights from state_dict *'.pt file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading model weights from {model_path} ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProgressiveGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0museGPU\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0museGPU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstoreAVG\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# , **config)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# model.eval()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/models/00019_PGGAN_CHEST_XRAY/model19/progressive_gan.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dimLatentVector, depthScale0, initBiasToZero, leakyness, perChannelNormalization, miniBatchStdDev, equalizedlR, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequalizedlR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mequalizedlR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mBaseGAN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimLatentVector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetNetG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/models/00019_PGGAN_CHEST_XRAY/model19/base_GAN.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dimLatentVector, dimOutput, useGPU, baseLearningRate, lossMode, attribKeysOrder, weightConditionD, weightConditionG, logisticGradReal, lambdaGP, epsilonD, GDPP, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# Move the networks to the gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateSolversDevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# Logistic loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/models/00019_PGGAN_CHEST_XRAY/model19/base_GAN.py\u001b[0m in \u001b[0;36mupdateSolversDevice\u001b[0;34m(self, buildAvG)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizerD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOptimizerD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizerG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOptimizerG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/models/00019_PGGAN_CHEST_XRAY/model19/progressive_gan.py\u001b[0m in \u001b[0;36mgetOptimizerD\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetOptimizerD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         return optim.Adam(filter(lambda p: p.requires_grad, self.netD.parameters()),\n\u001b[0m\u001b[1;32m     94\u001b[0m                           betas=[0, 0.99], lr=self.config.learningRate)\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbetas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbetas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         ):\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"betas must be either both floats or both Tensors\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbetas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcapturable\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mforeach\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: betas must be either both floats or both Tensors"]}]},{"cell_type":"markdown","source":["And we can run this function to view a selection of the images in a directory."],"metadata":{"id":"4SdWAMU9xu3z"}},{"cell_type":"code","source":["def view_images(directory, num_images=4, glob_rule=\"*\"):\n","    \"\"\"Displays a sample of images in the given directory\n","    They will display in rows of 4 images\n","    - directory: which directory to look for images\n","    - num_images: how many images to display (default 4, for one row)\n","    - glob_rule: argument to glob to filter images (default \"*\" selects all)\"\"\"\n","\n","    image_list = list(directory.glob(glob_rule))\n","    num_samples = min(num_images, len(image_list))\n","    images = [read_image(str(f)) for f in sorted(image_list)[:num_samples]]\n","    grid = make_grid(images, nrow=4, pad_value=255.0)\n","    return torchvision.transforms.ToPILImage()(grid)"],"metadata":{"id":"BGG1QVOuxwDR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["view_images(sample_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":287},"id":"XC_sJgEKx2Wk","executionInfo":{"status":"error","timestamp":1742796780731,"user_tz":-180,"elapsed":219,"user":{"displayName":"Irura Jack","userId":"08313893547887242679"}},"outputId":"e06e277f-50c4-4950-c084-5a1984b4008a"},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"unable to mmap 4096 bytes from file </content/drive/My Drive/Medical Data in Spain/sample_lung/batch_0>: No such device (19)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-aa9affb6747b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mview_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-20-5ef7d48ea024>\u001b[0m in \u001b[0;36mview_images\u001b[0;34m(directory, num_images, glob_rule)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mimage_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToPILImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-5ef7d48ea024>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mimage_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToPILImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/io/image.py\u001b[0m in \u001b[0;36mread_image\u001b[0;34m(path, mode, apply_exif_orientation)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_exif_orientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapply_exif_orientation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/io/image.py\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_torchbind_op_overload\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_must_dispatch_in_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_call_overload_packet_from_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;31m# TODO: use this to make a __dir__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: unable to mmap 4096 bytes from file </content/drive/My Drive/Medical Data in Spain/sample_lung/batch_0>: No such device (19)"]}]},{"cell_type":"markdown","source":["They look like chest x-rays, if a little strange. This is not unusual for a GAN, the images have basically the correct overall \"shape\" but the details aren't always right.\n","\n","For the rest of the lesson, we'll be working with model `10`, named `00010_FASTGAN_POLYP_PATCHES_W_MASKS`. Let's see what it produces."],"metadata":{"id":"5_ITjtfNx7x6"}},{"cell_type":"markdown","source":["**Task 5.3.8:** Generate $8$ images from model `10` and store them in the directory `output/polyp_samples`. Save the directory path in `polyp_dir`, we'll use it later. This may take a minute or so to run."],"metadata":{"id":"O7IRsZAgx8pp"}},{"cell_type":"code","source":["model_id = 10\n","polyp_dir = output_dir / \"polyp_sample\"\n","\n","generators.generate(model_id=10, num_samples=8, output_path = polyp_dir)\n","\n","print(\n","    f\"Created {len(list(polyp_dir.glob('*')))} images (should be twice the number asked for)\"\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":463},"id":"vsfM1Mgh27Fy","executionInfo":{"status":"error","timestamp":1742798282809,"user_tz":-180,"elapsed":216,"user":{"displayName":"Irura Jack","userId":"08313893547887242679"}},"outputId":"0390ea43-5047-4f92-c7b3-1b863b5524ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["ERROR:root:00010_FASTGAN_POLYP_PATCHES_W_MASKS: This model could not be added to model_executor list: 00010_FASTGAN_POLYP_PATCHES_W_MASKS: Some of the necessary dependencies (['torch', 'scipy', 'torchvision', 'opencv-contrib-python', 'pandas', 'easing-functions', 'scikit-image', 'matplotlib', 'ipdb', 'lmdb', 'numpy']) for this model are missing. Either set install_dependencies=True or manually run 'python src/medigan/install_model_dependencies.py --model_id 00010_FASTGAN_POLYP_PATCHES_W_MASKS' to install them. Error: The 'lmdb' distribution was not found and is required by the application\n"]},{"output_type":"error","ename":"Exception","evalue":"00010_FASTGAN_POLYP_PATCHES_W_MASKS: Some of the necessary dependencies (['torch', 'scipy', 'torchvision', 'opencv-contrib-python', 'pandas', 'easing-functions', 'scikit-image', 'matplotlib', 'ipdb', 'lmdb', 'numpy']) for this model are missing. Either set install_dependencies=True or manually run 'python src/medigan/install_model_dependencies.py --model_id 00010_FASTGAN_POLYP_PATCHES_W_MASKS' to install them. Error: The 'lmdb' distribution was not found and is required by the application","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mDistributionNotFound\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/medigan/execute_model/model_executor.py\u001b[0m in \u001b[0;36m_check_package_resources\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdependencies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             logging.debug(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mrequire\u001b[0;34m(self, *requirements)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \"\"\"\n\u001b[0;32m-> 1070\u001b[0;31m         \u001b[0mneeded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_requirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(self, requirements, env, installer, replace_conflicting, extras)\u001b[0m\n\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m             dist = self._resolve_dist(\n\u001b[0m\u001b[1;32m    898\u001b[0m                 \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace_conflicting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstaller\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequired_by\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_activate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_resolve_dist\u001b[0;34m(self, req, best, replace_conflicting, env, installer, required_by, to_activate)\u001b[0m\n\u001b[1;32m    937\u001b[0m                     \u001b[0mrequirers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequired_by\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mDistributionNotFound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequirers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    939\u001b[0m             \u001b[0mto_activate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mDistributionNotFound\u001b[0m: The 'lmdb' distribution was not found and is required by the application","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-13cca2f49d6a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpolyp_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"polyp_sample\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgenerators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolyp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m print(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/medigan/generators.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, model_id, num_samples, output_path, save_images, is_gen_function_returned, install_dependencies, **kwargs)\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mmodel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_model_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprovided_model_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m         model_executor = self.get_model_executor(\n\u001b[0m\u001b[1;32m    794\u001b[0m             \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstall_dependencies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstall_dependencies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/medigan/generators.py\u001b[0m in \u001b[0;36mget_model_executor\u001b[0;34m(self, model_id, install_dependencies)\u001b[0m\n\u001b[1;32m    752\u001b[0m                 \u001b[0;34mf\"{model_id}: This model could not be added to model_executor list: {e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             )\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     def generate(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/medigan/generators.py\u001b[0m in \u001b[0;36mget_model_executor\u001b[0;34m(self, model_id, install_dependencies)\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m             self.add_model_executor(\n\u001b[0m\u001b[1;32m    746\u001b[0m                 \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m                 \u001b[0minstall_dependencies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstall_dependencies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/medigan/generators.py\u001b[0m in \u001b[0;36madd_model_executor\u001b[0;34m(self, model_id, install_dependencies)\u001b[0m\n\u001b[1;32m    642\u001b[0m                 \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCONFIG_FILE_KEY_EXECUTION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m             )\n\u001b[0;32m--> 644\u001b[0;31m             self._add_model_executor(\n\u001b[0m\u001b[1;32m    645\u001b[0m                 \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m                 \u001b[0mexecution_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecution_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/medigan/generators.py\u001b[0m in \u001b[0;36m_add_model_executor\u001b[0;34m(self, model_id, execution_config, install_dependencies)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_model_executor_already_added\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m             model_executor = ModelExecutor(\n\u001b[0m\u001b[1;32m    671\u001b[0m                 \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m                 \u001b[0mexecution_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecution_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/medigan/execute_model/model_executor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_id, execution_config, download_package, install_dependencies)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeserialized_model_as_lib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_model_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_model_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/medigan/execute_model/model_executor.py\u001b[0m in \u001b[0;36m_setup_model_package\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m             ][CONFIG_FILE_KEY_GENERATE_ARGS_INPUT_LATENT_VECTOR_SIZE]\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_package_resources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_model_already_unpacked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_and_store_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/medigan/execute_model/model_executor.py\u001b[0m in \u001b[0;36m_check_package_resources\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 )\n\u001b[1;32m    166\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 raise Exception(\n\u001b[0m\u001b[1;32m    168\u001b[0m                     \u001b[0;34mf\"{self.model_id}: Some of the necessary dependencies ({self.dependencies}) for this model \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0;34mf\"are missing. Either set install_dependencies=True or manually run 'python src/medigan/install_model_dependencies.py --model_id {self.model_id}' to install them. Error: {e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mException\u001b[0m: 00010_FASTGAN_POLYP_PATCHES_W_MASKS: Some of the necessary dependencies (['torch', 'scipy', 'torchvision', 'opencv-contrib-python', 'pandas', 'easing-functions', 'scikit-image', 'matplotlib', 'ipdb', 'lmdb', 'numpy']) for this model are missing. Either set install_dependencies=True or manually run 'python src/medigan/install_model_dependencies.py --model_id 00010_FASTGAN_POLYP_PATCHES_W_MASKS' to install them. Error: The 'lmdb' distribution was not found and is required by the application"]}]},{"cell_type":"code","source":["output_dir"],"metadata":{"id":"113ognF44JY-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This actually generates two kinds of images: the medical image, and a \"mask\". This is why we have twice the number we asked for, as we get one image and its mask for each sample we ask for. The images are endoscope images of polyps, or growths. Here are a few."],"metadata":{"id":"iJZL4lWL4O1T"}},{"cell_type":"code","source":["view_images(polyp_dir, 8, \"*img*\")"],"metadata":{"id":"ONDEHf1B4Qks"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The masks mark which parts of the image are polyps. These generally indicate some sort of problem, and we want to be able to detect them. Here are the masks for the images we generated."],"metadata":{"id":"Z5ZHyEhl4YTa"}},{"cell_type":"code","source":["view_images(polyp_dir, 8, \"*mask*\")"],"metadata":{"id":"6d2Oqsgp4aBv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Depending on how your generation went, you could have mostly black images. These would be healthy images, where there was no problem. If so, you can re-run the generation to see what polyps and masks look like."],"metadata":{"id":"57IIXdXh4eg8"}},{"cell_type":"markdown","source":["### __Loading the Images__"],"metadata":{"id":"sAMtigmV4jQQ"}},{"cell_type":"markdown","source":["We can now generate medical images. These could have lots of uses. We could use them to test a system, or as examples for people to look at. One of their most common applications is data augmentation. When we don't have enough data to train a model, we need to get more. If we can't get more real data, we can use these models to get data that \"looks like\" real data. In practice, we'd usually use this artificial data to pre-train a model, then use real data to \"fine tune\" the model.\n"],"metadata":{"id":"QA3IoEUC4oud"}},{"cell_type":"markdown","source":["To enable us to do this, Medigan has provided us with `get_as_torch_dataloader`. This generates images and sets up a `DataLoader` for us, all in one step. We need to tell it which model to use (`model_id`), how many images to make in total (`num_samples`), how many in each batch (`batch_size`), and if we want to shuffle. As usual, we'll want to shuffle in training but not in validation. We'll also need to set `prefetch_factor=None` to avoid an error message.\n","\n","This makes a training set with $200$ images, with $4$ images in a batch. We'll keep using model `10`."],"metadata":{"id":"D-UlTlRZ4tTW"}},{"cell_type":"code","source":["train_dataloader = generators.get_as_torch_dataloader(\n","    model_id=10, num_samples=200, batch_size=4, shuffle=True, prefetch_factor=None\n",")"],"metadata":{"id":"BPx50tJF4ynH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We'll also need a validation data set."],"metadata":{"id":"5QTbpb1q48bN"}},{"cell_type":"markdown","source":["**Task 5.3.9:** Make a data loader for validation data. This time we'll only have 30 images, and we won't shuffle. All other settings should be the same."],"metadata":{"id":"GpGl_BEA49Wk"}},{"cell_type":"code","source":["val_dataloader = generators.get_as_torch_dataloader(\n","    model_id=10, num_samples=30,\n","    batch_size=4, shuffle=False,\n","    prefetch_factor=None\n",")\n","\n","test_batch = next(iter(val_dataloader))\n","test_batch_sample = test_batch[\"sample\"]\n","print(f\"Getting batches of shape {test_batch_sample.shape}\")"],"metadata":{"id":"6Gs7ivyL5B93"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<div class=\"alert alert-info\" role=\"alert\">\n","If you're wondering about that extra <tt>prefetch_factor</tt> argument, it has to do with multiple processing. Medigan is capable of creating several images simultaneously, which would speed up image generation at the cost of using more resources. If we turn that on by setting <tt>num_workers</tt> to a number greater than <tt>1</tt>, Medigan will create other small programs called workers that will run in the background and create images. These workers can \"prefetch\" images, meaning it generates them before they're needed, to further speed up the process. But we've chosen to not add this complexity, so we need to turn the prefetching off, since we don't have those extra workers to do it.\n","</div>"],"metadata":{"id":"uv0DzJJB6b9v"}},{"cell_type":"markdown","source":["We saw before that this model generates both endoscope images and masks for them. Our data loader does the same, by giving us a dictionary with a `sample` key and a `mask` key. But this time each is a `torch` tensor rather than an image file.\n","\n","Let's look at an example of each. If your mask is all black for a healthy image, you can run the previous cell again to get a fresh batch."],"metadata":{"id":"3Rxvo4Nt6ocz"}},{"cell_type":"code","source":["endoscope = test_batch[\"sample\"]\n","mask = test_batch[\"mask\"]\n","print(f\"Endoscope image shape: {endoscope[0].shape}\")\n","print(f\"Mask image shape: {mask[0].shape}\")\n","print()\n","\n","print(\"Endoscope\")\n","plt.imshow(endoscope[0])\n","plt.axis(\"off\")\n","plt.show()\n","\n","print(\"Mask\")\n","plt.imshow(mask[0])\n","plt.axis(\"off\")\n","plt.show()"],"metadata":{"id":"HZYrK6kz6x8x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Looking deeper, the mask data is only `0` or `1`. This will make it a good label for a classifier. We can check that with `unique` — it will tell us all the different values in our tensor."],"metadata":{"id":"BfKArKLe64G9"}},{"cell_type":"markdown","source":["**Task 5.3.10:** Call the `unique` method of the `mask` tensor to verify it only has `0` and `1`."],"metadata":{"id":"tkc9RMu665YA"}},{"cell_type":"code","source":["mask.unique()"],"metadata":{"id":"K8err-7X699C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __Training a Model__"],"metadata":{"id":"GBjrT7l271FR"}},{"cell_type":"markdown","source":["We can use these data loaders to train a model. This model won't be a GAN, we already have one of those. Instead, we'll train a model that detects polyps on endoscope images. We'll use the images as the data, and the mask as the label. This should give us a model that takes in an image, and returns a mask highlighting any polyps. This would be quite useful to the medical industry, to speed up the processing of endoscope results and to double check in case the doctor may have missed something. Our GAN images aren't real data, but they're close enough to get a model doing the right low-level processing.\n","\n","We could train a model from scratch, but as usual that isn't a good use of our time and resources. We'll do transfer learning, since there are many good pre-trained models that do this sort of detection of parts of an image. This process is called segmentation. A popular choice is `DeepLabV3`, so we'll use that. It's built into `torchvision`, we can load it in two lines."],"metadata":{"id":"HZwAeIuv78wh"}},{"cell_type":"code","source":["pretrained_weights = (\n","    torchvision.models.segmentation.DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1\n",")\n","model = torchvision.models.segmentation.deeplabv3_resnet50(weights=pretrained_weights)"],"metadata":{"id":"3ePLLIQn8Hx8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is a very large convolutional network, ending in a classifier. Let's look at the model overall. Note the rather large classifier at the end, you can ignore the `aux_classifier` as we won't be using it."],"metadata":{"id":"8muX75FR8JLT"}},{"cell_type":"code","source":["print(model)"],"metadata":{"id":"I4_Snhmz8Ngc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's see what shape it gives us. We have a `test_batch_sample` from a few cells ago, we can run that through the model. You may have noticed the shape is a typical image shape of `[4, 256, 256, 3]`, but torch likes tensors of shape `[4, 3, 256, 256]` (so the channels before the height and width). Also, the model has two outputs: `out` and `aux`. We'll only use `out`."],"metadata":{"id":"3PCJdp3V8Ufw"}},{"cell_type":"code","source":["test_batch_sample.shape"],"metadata":{"id":"H-N-lAgd8ZYU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Task 5.3.11:** Move the `test_batch_sample` to GPU, and get the shape of the `out` tensor. The code here takes care of adjusting the tensor to have channel in the second slot, running the model, and selecting the `out` output."],"metadata":{"id":"nboWq1LZ8eyO"}},{"cell_type":"code","source":["sample_cuda = test_batch_sample.to(device)\n","\n","model.to(device)\n","moved_channel = sample_cuda.permute(0, 3, 1, 2)\n","result = model(moved_channel)\n","out = result[\"out\"]\n","\n","output_shape = out.shape\n","print(f\"Output shape: {output_shape}\")"],"metadata":{"id":"X6jCy-N98ggg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is _almost_ what we need. We need it to match our mask, which only has one channel. The mask tensor actually has three, but they're all the same, so we'll only use the first one."],"metadata":{"id":"_oeB0cmf_HKV"}},{"cell_type":"code","source":["print(f\"Shape from GAN: {test_batch['mask'].shape}\")\n","print()\n","moved_channel_mask = test_batch[\"mask\"][:, :, :, :1].permute(0, 3, 1, 2)\n","print(f\"Shape we'll actually use: {moved_channel_mask.shape}\")\n","print()"],"metadata":{"id":"eCearA8i_S36"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We'll need to do our usual process for transfer learning and modify the classifier part of the model. To minimize the training we'll need, we'll only replace the last layer. What is it currently?"],"metadata":{"id":"c-pEvYOU_dui"}},{"cell_type":"code","source":["model.classifier[-1]"],"metadata":{"id":"bAT3oS31_isU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is a convolutional layer that takes $256$ channels in and gives $21$ channels out. A convolutional layer with a $1$ x $1$ kernel doesn't change the height or width of the image, but does change the number of channels. Since we're already getting the right height and width, we should only change the number of out channels.\n","\n","We'll want to replace this with another convolutional layer with the same kernel, but only one channel output."],"metadata":{"id":"-7ChAVmq_n3N"}},{"cell_type":"markdown","source":["**Task 5.3.12:** Create a convolutional layer that takes in `256` channels and outputs `1` channel, with a $1$ x $1$ kernel. Replace the classifier's final layer with it."],"metadata":{"id":"ofannr2Z_tEC"}},{"cell_type":"code","source":["new_final_layer = torch.nn.Conv2d(in_channels=256, out_channels=1, kernel_size=(1,1))\n","\n","model.classifier[-1] = new_final_layer\n","\n","model.to(device)\n","print(f\"New output shape: {model(moved_channel)['out'].shape}\")"],"metadata":{"id":"uMGKmOZD_uoD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["That's what we wanted. Now we need to train. Unfortunately, we can't use the training code from previous projects, as we need to separate the `sample` and `mask`, and change the order from `[batch, height, width, channel]` to `[batch, channel, height, weight]`. Also, we can't really compute the \"accuracy\", since our target isn't a single value, but rather a whole image. We'll only have the loss. Here is a training function that has those modifications."],"metadata":{"id":"KiTFjbk_BHX0"}},{"cell_type":"code","source":["def train_model(\n","    model, train_dataloader, val_dataloader, criterion, optimizer, num_epochs\n","):\n","    for epoch in range(num_epochs):\n","        # Training phase\n","        model.train()\n","        running_loss = 0.0\n","        for data_dict in tqdm(train_dataloader):\n","            inputs = data_dict.get(\"sample\")\n","            masks = data_dict.get(\"mask\")\n","            # Reorder dimensions to [batch_size, channels, height, width]\n","            inputs = inputs.permute(0, 3, 1, 2)\n","            masks = masks.permute(0, 3, 1, 2)\n","            # Reduce mask to single channel\n","            masks = masks[:, :1, :, :]\n","\n","            inputs = inputs.to(device)\n","            masks = masks.to(device)\n","            optimizer.zero_grad()\n","\n","            # Select classifier output\n","            outputs = model(inputs)[\"out\"]\n","\n","            # Compute loss and step optimizer\n","            loss = criterion(outputs, masks)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item() * inputs.size(0)\n","\n","        epoch_loss = running_loss / len(train_dataloader.dataset)\n","        print(f\"Epoch {epoch}/{num_epochs - 1}, Train Loss: {epoch_loss:.4f}\")\n","\n","        # Validation phase\n","        model.eval()\n","        val_loss = 0.0\n","        with torch.no_grad():\n","            for data_dict in tqdm(val_dataloader):\n","                inputs = data_dict.get(\"sample\")\n","                masks = data_dict.get(\"mask\")\n","                inputs = inputs.permute(0, 3, 1, 2)\n","                masks = masks.permute(0, 3, 1, 2)\n","                masks = masks[:, :1, :, :]\n","\n","                inputs = inputs.to(device)\n","                masks = masks.to(device)\n","\n","                outputs = model(inputs)[\"out\"]\n","                loss = criterion(outputs, masks)\n","                val_loss += loss.item() * inputs.size(0)\n","\n","        val_epoch_loss = val_loss / len(val_dataloader.dataset)\n","        print(f\"Validation Loss: {val_epoch_loss:.4f}\")"],"metadata":{"id":"VnWs-D01BPCO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Our function takes a few arguments: `model`, `train_dataloader`, `val_dataloader`, `criterion`, `optimizer`, `num_epochs`. We have the first three already, now we need the `criterion` and the `optimizer`, and we'll need to decide how many epochs to train.\n","\n","The `criterion` is our loss function. We'll use the Binary Cross Entropy again. But this time we're getting the raw logit output, rather than values between $0$ and $1$. The right one to use here is `BCEWithLogitsLoss`.\n","\n","We need to create an optimizer as well. We'll use `Adam` again, as it's often a good choice. After some experimenting, we found this model is a bit prone to overfitting, so we'll slow the training down."],"metadata":{"id":"oTCtDAgIBUE5"}},{"cell_type":"markdown","source":["**Task 5.3.13:** Fill in the code to create an `Adam` optimizer with the learning rate (`lr`) set to `0.0001`."],"metadata":{"id":"LlzWVJj5BZI3"}},{"cell_type":"code","source":["# Define loss function and optimizer\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","\n","print(\n","    f\"Created optimizer of type {type(optimizer)} with learning rate {optimizer.defaults['lr']}\"\n",")"],"metadata":{"id":"_VK_r0d0BdPM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<div class=\"alert alert-info\" role=\"alert\">\n","    <p><b>Changes with respect to the video</b></p>\n","<p>The instructor in the video is training the model for 2 epochs before loading the fully trained model (in the next step).</p>\n","    <p>Given that training the models take a long time, we have since removed this step, you can safely proceed to the next section in which the model is loaded from <code>polyp_model.pth</code></p>\n","</div>"],"metadata":{"id":"OphXZaexDD5P"}},{"cell_type":"markdown","source":["**Task 5.3.14:** Load the pre-trained model from `polyp_model.pth`"],"metadata":{"id":"gb4F2jXUDGZP"}},{"cell_type":"code","source":["model_state = torch.load(\"polyp_model.pth\")\n","\n","# Load the state dictionary that has trained weights\n","model.load_state_dict(model_state)"],"metadata":{"id":"mrviYoEeDLgT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<div class=\"alert alert-info\" role=\"alert\">\n","You may have noticed that we didn't turn off training on the weights of the pre-trained model. This was intentional, as we found that if we did that the model performed quite poorly. In this situation, we needed to not just change the classification, but also tune the larger model. This is not uncommon. It often happens when what we want to do is a bit outside of what the model did originally, or when our data is sufficiently different from what the original model was trained on. In this case, it's more the latter. The model we're using was trained on a variety of full color images of people, animals, vehicles, etc. So we're specializing it a fair bit. And we found, again through experimenting, that it didn't take <i>too</i> much longer to train this way.\n","</div>"],"metadata":{"id":"tPqC_Vt8DXZr"}},{"cell_type":"markdown","source":["### __Testing our Model__"],"metadata":{"id":"9HOUTt44Dd-w"}},{"cell_type":"markdown","source":["Great, we have a trained model! But we should make sure it's doing what we want.\n","\n","Let's generate some new images to test it out."],"metadata":{"id":"FBt9PbXTDlL9"}},{"cell_type":"markdown","source":["**Task 5.3.15:** Create another data loader for model `10`. Only create $20$ images this time, in batches of $4$, without shuffling."],"metadata":{"id":"CsnjQ6cHDprt"}},{"cell_type":"code","source":["test_dataloader = generators.get_as_torch_dataloader(model_id=10, num_samples=20, batch_size=4,\n","                                                     shuffle=False, prefetch_factor=None)\n","\n","final_test_batch = next(iter(test_dataloader))\n","final_test_sample = final_test_batch[\"sample\"]\n","final_test_mask = final_test_batch[\"mask\"]\n","\n","print(f\"Test sample image shape: {final_test_sample.shape}\")\n","print(f\"Test mask image shape: {final_test_mask.shape}\")"],"metadata":{"id":"W3R9ru5kDt1M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The previous cell pulled a batch from the loader. The data was separated into the images (`final_test_sample`) and the masks (`final_test_mask`). Now we need the output of the model to compare. We'll need to move the channels on the input, and select the `out` part of the output."],"metadata":{"id":"Uqodf18ZEm7A"}},{"cell_type":"markdown","source":["**Task 5.3.16:** Fill in the missing parts to run the model on our `final_test_sample`. You'll need to `permute` the channel to the right place, move it to `device`, run the model on it, and select the `out` part of the output."],"metadata":{"id":"QCbFsW6QEusL"}},{"cell_type":"code","source":["# Permute the channel to the right place\n","final_moved_channels = final_test_sample.permute(0, 3, 1, 2)\n","\n","# Move to the device (GPU)\n","final_moved_channels_cuda = final_moved_channels.to(device)\n","\n","# Run the model\n","final_model_result = model(final_moved_channels_cuda)\n","\n","# Select the out part of the result\n","final_out = final_model_result['out']\n","\n","# Move the channel back to the last position so we can display the image\n","final_classifier_out = final_out.permute(0, 2, 3, 1)\n","\n","print(f\"Final images shape {final_classifier_out.shape}\")"],"metadata":{"id":"iEVmf6DWEy8M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If everything went according to plan, we should now have a `[4, 256, 256, 1]` tensor. That's four images. But we're not quite getting images, as we're getting the raw logits. We want a classifier type output, with values between $0$ and $1$. We're getting something else."],"metadata":{"id":"BUmw8ZAzGhzO"}},{"cell_type":"code","source":["print(\"Getting range:\")\n","final_classifier_out.min().item(), final_classifier_out.max().item()"],"metadata":{"id":"a-i-sVrJGmzF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can repair that by running the result through a sigmoid function, which will ensure every value is between $0$ and $1$."],"metadata":{"id":"4Cl70A4gGjLL"}},{"cell_type":"code","source":["final_classifier_image = torch.sigmoid(final_classifier_out)"],"metadata":{"id":"C14RKRtoGuoc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We're ready for the final reveal!\n","\n","Here's a function that will plot the images in a torch tensor, in rows of $4$."],"metadata":{"id":"Ki6YLLz6GzIv"}},{"cell_type":"code","source":["def plot_images_from_tensor(tensor):\n","    grid = make_grid(tensor.permute(0, 3, 1, 2), nrow=4, pad_value=1.0)\n","    return torchvision.transforms.ToPILImage()(grid)"],"metadata":{"id":"amkhdSZUG3fL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can run this on our `final_test_sample` to see the input endoscope images."],"metadata":{"id":"zBqJfdziG77v"}},{"cell_type":"code","source":["plot_images_from_tensor(final_test_sample)"],"metadata":{"id":"sOy503F1G_4z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This same function will work on our masks. We can compare the mask that we expect to be correct (`final_test_mask`) with our output (`final_classifier_image`) to see how our model performed."],"metadata":{"id":"JYygHsHmHD52"}},{"cell_type":"markdown","source":["**Task 5.3.17:** Run the `plot_images_from_tensor` on the `final_test_mask` and `final_classifier_image`. You'll need to do it in different cells."],"metadata":{"id":"VDz8McEvHMi5"}},{"cell_type":"code","source":["# Plot the final_test_mask — the \"right answer\"\n","plot_images_from_tensor(final_test_mask)"],"metadata":{"id":"mFFuZoYNIYnP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot the final_classifier_image - what our model outputs\n","plot_images_from_tensor(final_classifier_image)"],"metadata":{"id":"Nx_Lm7BKIZl4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Learnt\n","\n","- We can get pre-trained GANs to generate many different kinds of data\n","- They are often used to augment data when we don't have enough real data\n","- We can use them as a form of pre-training to get a model ready for real data\n","- We can even do this to tune an already pre-trained model\n","- This lets us train a model quickly with limited data"],"metadata":{"id":"ccORm68kIfew"}}]}